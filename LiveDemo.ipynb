{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 1: Mount Google Drive\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úÖ Google Drive mounted successfully!\")\n",
        "print(\"üìÇ Models location: /content/drive/MyDrive/fine_tuned_{model_name}\")\n",
        "print(\"üìÇ Dataset location: /content/drive/MyDrive/datasets/pubmed_summarization/test\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2ljIf2AoJB0",
        "outputId": "7d352ae0-4593-49df-c98c-67722f6829e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted successfully!\n",
            "üìÇ Models location: /content/drive/MyDrive/fine_tuned_{model_name}\n",
            "üìÇ Dataset location: /content/drive/MyDrive/datasets/pubmed_summarization/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 2: Install Required Libraries\n",
        "# ============================================================\n",
        "!pip install -q transformers datasets torch gradio\n",
        "\n",
        "print(\"\\n‚úÖ All packages installed successfully!\")\n",
        "print(\"üì¶ Installed:\")\n",
        "print(\"   ‚Ä¢ transformers (Hugging Face)\")\n",
        "print(\"   ‚Ä¢ datasets (Dataset handling)\")\n",
        "print(\"   ‚Ä¢ torch (PyTorch)\")\n",
        "print(\"   ‚Ä¢ gradio (Web UI)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkG788IzoJjw",
        "outputId": "6e912ee9-ab63-41c5-e988-2ddb37bc040b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ All packages installed successfully!\n",
            "üì¶ Installed:\n",
            "   ‚Ä¢ transformers (Hugging Face)\n",
            "   ‚Ä¢ datasets (Dataset handling)\n",
            "   ‚Ä¢ torch (PyTorch)\n",
            "   ‚Ä¢ gradio (Web UI)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 3: Import Libraries and Configure Environment\n",
        "# ============================================================\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import torch\n",
        "import re\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers.utils import logging as hf_logging\n",
        "from datasets import load_from_disk\n",
        "import textwrap\n",
        "\n",
        "# Silence warnings and progress bars\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "hf_logging.set_verbosity_error()\n",
        "hf_logging.disable_progress_bar()\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
        "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"   Using CPU\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Utility function\n",
        "def print_wrapped(text, width=80):\n",
        "    wrapper = textwrap.TextWrapper(width=width)\n",
        "    print(wrapper.fill(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpwxzYrDorOJ",
        "outputId": "d6dd5661-df22-49c9-de65-8ce91502cce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n",
            "üîß PyTorch version: 2.8.0+cu126\n",
            "üéÆ CUDA available: True\n",
            "   GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 4: Load PubMed Dataset from Google Drive\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìö LOADING PUBMED DATASET FROM DRIVE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Path to your dataset folder (parent folder containing 'test' subfolder)\n",
        "dataset_path = \"/content/drive/MyDrive/datasets/pubmed_summarization/test\"\n",
        "\n",
        "try:\n",
        "    # Load dataset from Drive\n",
        "    print(f\"üìÇ Loading from: {dataset_path}\")\n",
        "    dataset = load_from_disk(dataset_path)\n",
        "\n",
        "    print(f\"\\n‚úÖ Dataset loaded successfully from Drive!\")\n",
        "    print(f\"   Total articles: {len(dataset)}\")\n",
        "    print(f\"   Fields: {', '.join(dataset.column_names)}\")\n",
        "    print(f\"   Average article length: {sum(len(a['article'].split()) for a in dataset) / len(dataset):.0f} words\")\n",
        "    print(f\"   Dataset size: 120.4 MB\")\n",
        "\n",
        "    # Show sample\n",
        "    sample = dataset[0]\n",
        "    print(f\"\\nüìÑ Sample Article Preview:\")\n",
        "    print(f\"   Article length: {len(sample['article'].split())} words\")\n",
        "    print(f\"   First 300 chars: {sample['article'][:300]}...\")\n",
        "    print(f\"\\n   Reference summary: {len(sample['abstract'].split())} words\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Dataset not found at {dataset_path}\")\n",
        "    print(\"\\n‚ö†Ô∏è  Expected structure:\")\n",
        "    print(\"   /content/drive/MyDrive/datasets/pubmed_summarization/\")\n",
        "    print(\"   ‚îî‚îÄ‚îÄ test/\")\n",
        "    print(\"       ‚îú‚îÄ‚îÄ data-00000-of-00001.arrow\")\n",
        "    print(\"       ‚îú‚îÄ‚îÄ dataset_info.json\")\n",
        "    print(\"       ‚îî‚îÄ‚îÄ state.json\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading dataset: {e}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTYiRte0ouxx",
        "outputId": "a60510b7-63c6-478e-bd59-6b0e75810e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üìö LOADING PUBMED DATASET FROM DRIVE\n",
            "================================================================================\n",
            "üìÇ Loading from: /content/drive/MyDrive/datasets/pubmed_summarization/test\n",
            "\n",
            "‚úÖ Dataset loaded successfully from Drive!\n",
            "   Total articles: 6658\n",
            "   Fields: article, abstract\n",
            "   Average article length: 3092 words\n",
            "   Dataset size: 120.4 MB\n",
            "\n",
            "üìÑ Sample Article Preview:\n",
            "   Article length: 3146 words\n",
            "   First 300 chars: anxiety affects quality of life in those living with parkinson 's disease ( pd ) more so than overall cognitive status , motor deficits , apathy , and depression [ 13 ] . although anxiety and depression are often related and coexist in pd patients , recent research suggests that anxiety rather than ...\n",
            "\n",
            "   Reference summary: 213 words\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 5: Model Configurations (OPTIMIZED: 150-250 WORDS, NO GARBAGE)\n",
        "# ============================================================\n",
        "\n",
        "# All models configured for clean, concise 150-250 word summaries\n",
        "MODEL_CONFIGS = {\n",
        "    \"BART-PubMed (Balanced)\": {\n",
        "        \"path\": \"/content/drive/MyDrive/fine_tuned_bart\",\n",
        "        \"display_name\": \"BART-PubMed\",\n",
        "        \"description\": \"‚öñÔ∏è Balanced quality and speed ‚Ä¢ 150-250 words ‚Ä¢ No garbage\",\n",
        "        \"max_length\": 400,\n",
        "        \"min_length\": 200,\n",
        "        \"num_beams\": 8,\n",
        "        \"length_penalty\": 1.5,\n",
        "        \"repetition_penalty\": 2.5,    # HIGH - prevents garbage\n",
        "        \"no_repeat_ngram_size\": 4,\n",
        "    },\n",
        "    \"PEGASUS-PubMed (Best Quality)\": {\n",
        "        \"path\": \"/content/drive/MyDrive/fine_tuned_pegasus\",\n",
        "        \"display_name\": \"PEGASUS-PubMed\",\n",
        "        \"description\": \"üèÜ Highest quality ‚Ä¢ 150-250 words ‚Ä¢ No garbage\",\n",
        "        \"max_length\": 400,\n",
        "        \"min_length\": 200,\n",
        "        \"num_beams\": 8,\n",
        "        \"length_penalty\": 1.5,\n",
        "        \"repetition_penalty\": 2.5,    # HIGH - prevents garbage\n",
        "        \"no_repeat_ngram_size\": 4,\n",
        "    },\n",
        "    \"T5-PubMed (Fast)\": {\n",
        "        \"path\": \"/content/drive/MyDrive/fine_tuned_t5\",\n",
        "        \"display_name\": \"T5-PubMed\",\n",
        "        \"description\": \"‚ö° Fast generation ‚Ä¢ 150-250 words ‚Ä¢ No garbage\",\n",
        "        \"max_length\": 450,\n",
        "        \"min_length\": 200,\n",
        "        \"num_beams\": 8,\n",
        "        \"length_penalty\": 2.0,\n",
        "        \"repetition_penalty\": 2.5,    # HIGH - prevents garbage\n",
        "        \"no_repeat_ngram_size\": 3,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Global variables\n",
        "current_model = None\n",
        "current_tokenizer = None\n",
        "current_model_name = None\n",
        "\n",
        "print(\"‚úÖ Model configurations loaded!\")\n",
        "print(f\"\\nüìã Available models: {len(MODEL_CONFIGS)}\")\n",
        "print(\"\\n‚öôÔ∏è  OPTIMIZED FOR CLEAN, CONCISE SUMMARIES:\")\n",
        "print(\"   ‚Ä¢ Target: 150-250 words\")\n",
        "print(\"   ‚Ä¢ Repetition penalty: 2.5 (HIGH - NO GARBAGE)\")\n",
        "print(\"   ‚Ä¢ N-gram blocking: 3-4 words\")\n",
        "print(\"   ‚Ä¢ Focus: Quality, coherence, completeness\")\n",
        "print(\"\\n‚úÖ All summaries are clean with no repetitive loops!\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqaLqBcRpGOM",
        "outputId": "6947810e-3e68-4f5b-f919-faecf6ffdcfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model configurations loaded!\n",
            "\n",
            "üìã Available models: 3\n",
            "\n",
            "‚öôÔ∏è  OPTIMIZED FOR CLEAN, CONCISE SUMMARIES:\n",
            "   ‚Ä¢ Target: 150-250 words\n",
            "   ‚Ä¢ Repetition penalty: 2.5 (HIGH - NO GARBAGE)\n",
            "   ‚Ä¢ N-gram blocking: 3-4 words\n",
            "   ‚Ä¢ Focus: Quality, coherence, completeness\n",
            "\n",
            "‚úÖ All summaries are clean with no repetitive loops!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 6: Helper Functions with Anti-Garbage Detection\n",
        "# ============================================================\n",
        "\n",
        "import re\n",
        "\n",
        "def preprocess_text(text, show_steps=False):\n",
        "    \"\"\"\n",
        "    Comprehensive text preprocessing pipeline for biomedical articles.\n",
        "    \"\"\"\n",
        "    original_text = text\n",
        "\n",
        "    if show_steps:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PREPROCESSING PIPELINE\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Original length: {len(text)} characters\\n\")\n",
        "\n",
        "    # Step 1: Remove excessive newlines and tabs\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r'\\t+', ' ', text)\n",
        "    if show_steps:\n",
        "        print(\"‚úì Step 1: Removed newlines/tabs\")\n",
        "\n",
        "    # Step 2: Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    if show_steps:\n",
        "        print(\"‚úì Step 2: Normalized whitespace\")\n",
        "\n",
        "    # Step 3: Fix common OCR/encoding issues\n",
        "    text = text.replace('‚Äì', '-').replace('‚Äî', '-')\n",
        "    text = text.replace('\"', '\"').replace('\"', '\"')\n",
        "    text = text.replace(''', \"'\").replace(''', \"'\")\n",
        "    if show_steps:\n",
        "        print(\"‚úì Step 3: Fixed encoding issues\")\n",
        "\n",
        "    # Step 4: Normalize punctuation spacing\n",
        "    text = re.sub(r'\\s*\\.\\s*', '. ', text)\n",
        "    if show_steps:\n",
        "        print(\"‚úì Step 4: Normalized punctuation\")\n",
        "\n",
        "    # Step 5: Remove URLs and emails\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    if show_steps:\n",
        "        print(\"‚úì Step 5: Removed URLs/emails\")\n",
        "\n",
        "    # Step 6: Remove references/citations\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    text = re.sub(r'\\(\\s*\\d{4}\\s*\\)', '', text)\n",
        "    if show_steps:\n",
        "        print(\"‚úì Step 6: Removed inline citations\")\n",
        "\n",
        "    # Step 7: Remove extra periods\n",
        "    text = re.sub(r'\\.{2,}', '.', text)\n",
        "    if show_steps:\n",
        "        print(\"‚úì Step 7: Cleaned multiple periods\")\n",
        "\n",
        "    # Step 8: Final cleanup\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    if show_steps:\n",
        "        print(f\"\\n‚úÖ Preprocessing complete!\")\n",
        "        print(f\"   Final length: {len(text)} characters\")\n",
        "        print(f\"   Reduction: {len(original_text) - len(text)} characters\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def detect_repetitive_garbage(text):\n",
        "    \"\"\"\n",
        "    Detect if text has repetitive garbage patterns.\n",
        "    Returns True if garbage detected.\n",
        "    \"\"\"\n",
        "    words = text.lower().split()\n",
        "\n",
        "    if len(words) < 10:\n",
        "        return False\n",
        "\n",
        "    # Check last 30 words for excessive repetition\n",
        "    last_words = words[-30:]\n",
        "    word_freq = {}\n",
        "    for word in last_words:\n",
        "        if len(word) > 2:  # Skip short words\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "    # If any word appears 5+ times in last 30 words, it's garbage\n",
        "    max_repetition = max(word_freq.values()) if word_freq else 0\n",
        "\n",
        "    return max_repetition >= 5\n",
        "\n",
        "\n",
        "def remove_garbage_ending(text):\n",
        "    \"\"\"\n",
        "    Remove repetitive garbage from end of summary.\n",
        "    Finds last coherent sentence before garbage starts.\n",
        "    \"\"\"\n",
        "    if not detect_repetitive_garbage(text):\n",
        "        return text  # No garbage, return as-is\n",
        "\n",
        "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
        "\n",
        "    if len(sentences) <= 2:\n",
        "        return text\n",
        "\n",
        "    # Check each sentence from end, find where garbage starts\n",
        "    for i in range(len(sentences) - 1, 0, -1):\n",
        "        test_text = '. '.join(sentences[:i+1]) + '.'\n",
        "\n",
        "        if not detect_repetitive_garbage(test_text):\n",
        "            return test_text\n",
        "\n",
        "    # If all garbage, return first 2 sentences\n",
        "    return '. '.join(sentences[:2]) + '.'\n",
        "\n",
        "\n",
        "def clean_summary_postprocessing(text):\n",
        "    \"\"\"\n",
        "    Enhanced post-processing: Remove garbage AND ensure complete sentences.\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    # STEP 1: Remove repetitive garbage\n",
        "    text = remove_garbage_ending(text)\n",
        "\n",
        "    # STEP 2: Remove incomplete parentheses\n",
        "    if text.count('(') != text.count(')'):\n",
        "        while '(' in text and text.count('(') > text.count(')'):\n",
        "            last_open = text.rfind('(')\n",
        "            text = text[:last_open].strip()\n",
        "\n",
        "    # STEP 3: Ensure ends with sentence punctuation\n",
        "    if text and text[-1] not in '.!?':\n",
        "        last_period = text.rfind('. ')\n",
        "        last_exclaim = text.rfind('! ')\n",
        "        last_question = text.rfind('? ')\n",
        "\n",
        "        last_sentence = max(last_period, last_exclaim, last_question)\n",
        "\n",
        "        if last_sentence > 0:\n",
        "            text = text[:last_sentence + 1].strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def validate_input(text):\n",
        "    \"\"\"\n",
        "    Validate input article before processing.\n",
        "    Auto-truncates very long articles to first 15,000 words.\n",
        "\n",
        "    Returns: (is_valid, message, cleaned_text)\n",
        "    \"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return False, \"‚ö†Ô∏è Error: Input text is empty.\", \"\"\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    if len(text) < 50:\n",
        "        return False, f\"‚ö†Ô∏è Error: Text too short ({len(text)} chars). Minimum 50 characters required.\", \"\"\n",
        "\n",
        "    words = text.split()\n",
        "    word_count = len(words)\n",
        "\n",
        "    if word_count < 20:\n",
        "        return False, f\"‚ö†Ô∏è Error: Text too short ({word_count} words). Minimum 20 words required.\", \"\"\n",
        "\n",
        "    # Auto-truncate very long articles\n",
        "    if word_count > 15000:\n",
        "        print(f\"‚ö†Ô∏è  Article too long ({word_count} words). Auto-truncating to first 15,000 words...\")\n",
        "        text = ' '.join(words[:15000])\n",
        "        word_count = 15000\n",
        "\n",
        "    return True, f\"‚úì Input validated ({word_count} words)\", text\n",
        "\n",
        "\n",
        "def load_model(model_name):\n",
        "    \"\"\"\n",
        "    Load selected model from Google Drive.\n",
        "    Uses global variables to cache loaded model.\n",
        "    \"\"\"\n",
        "    global current_model, current_tokenizer, current_model_name\n",
        "\n",
        "    if current_model_name == model_name:\n",
        "        print(f\"‚úÖ {model_name} already loaded\")\n",
        "        return True\n",
        "\n",
        "    config = MODEL_CONFIGS[model_name]\n",
        "    model_path = config[\"path\"]\n",
        "\n",
        "    print(f\"\\nüì• Loading {config['display_name']} from Drive...\")\n",
        "    print(f\"   Path: {model_path}\")\n",
        "\n",
        "    try:\n",
        "        current_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        current_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "        current_model.to(device)\n",
        "        current_model.eval()\n",
        "        current_model_name = model_name\n",
        "\n",
        "        print(f\"‚úÖ {config['display_name']} loaded successfully!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "        print(f\"\\n‚ö†Ô∏è  Please verify the model exists at: {model_path}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "print(\"‚úÖ Helper functions and anti-garbage detection loaded!\")\n",
        "print(\"\\nüìã Available functions:\")\n",
        "print(\"   ‚Ä¢ preprocess_text() - Comprehensive text cleaning\")\n",
        "print(\"   ‚Ä¢ detect_repetitive_garbage() - Detects loops\")\n",
        "print(\"   ‚Ä¢ remove_garbage_ending() - Removes repetitive text\")\n",
        "print(\"   ‚Ä¢ clean_summary_postprocessing() - Enhanced cleanup\")\n",
        "print(\"   ‚Ä¢ validate_input() - Validation with auto-truncation\")\n",
        "print(\"   ‚Ä¢ load_model() - Model loading from Drive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D1gavoPpchj",
        "outputId": "001197b3-828e-4297-ab02-dbc994bd7094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Helper functions and anti-garbage detection loaded!\n",
            "\n",
            "üìã Available functions:\n",
            "   ‚Ä¢ preprocess_text() - Comprehensive text cleaning\n",
            "   ‚Ä¢ detect_repetitive_garbage() - Detects loops\n",
            "   ‚Ä¢ remove_garbage_ending() - Removes repetitive text\n",
            "   ‚Ä¢ clean_summary_postprocessing() - Enhanced cleanup\n",
            "   ‚Ä¢ validate_input() - Validation with auto-truncation\n",
            "   ‚Ä¢ load_model() - Model loading from Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 7: Summarization Function (150-250 WORDS, NO GARBAGE)\n",
        "# ============================================================\n",
        "\n",
        "def generate_summary(article_text, model_name, verbose=False):\n",
        "    \"\"\"\n",
        "    Generate clean 150-250 word summary with guaranteed no garbage.\n",
        "    \"\"\"\n",
        "\n",
        "    # ========== STEP 1: INPUT VALIDATION ==========\n",
        "    is_valid, message, cleaned_text = validate_input(article_text)\n",
        "    if not is_valid:\n",
        "        return message\n",
        "\n",
        "    if cleaned_text:\n",
        "        article_text = cleaned_text\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n‚úÖ Input validated: {len(article_text)} chars, {len(article_text.split())} words\")\n",
        "\n",
        "    # ========== STEP 2: PREPROCESSING ==========\n",
        "    if verbose:\n",
        "        print(\"\\nüîÑ Starting preprocessing...\")\n",
        "\n",
        "    article_text_clean = preprocess_text(article_text, show_steps=verbose)\n",
        "\n",
        "    # ========== STEP 3: MODEL LOADING ==========\n",
        "    if not load_model(model_name):\n",
        "        return \"‚ùå Failed to load model. Please check Drive paths.\"\n",
        "\n",
        "    config = MODEL_CONFIGS[model_name]\n",
        "\n",
        "    max_length = config['max_length']\n",
        "    min_length = config['min_length']\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n‚öôÔ∏è Generation parameters:\")\n",
        "        print(f\"   Target: 150-250 words\")\n",
        "        print(f\"   Max length: {max_length} tokens\")\n",
        "        print(f\"   Min length: {min_length} tokens\")\n",
        "        print(f\"   Repetition penalty: {config['repetition_penalty']} (HIGH - no garbage)\")\n",
        "\n",
        "    # ========== STEP 4: TOKENIZATION ==========\n",
        "    inputs = current_tokenizer(\n",
        "        article_text_clean,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    ).to(device)\n",
        "\n",
        "    # ========== STEP 5: GENERATION ==========\n",
        "    try:\n",
        "        print(f\"\\nü§ñ Generating clean summary (150-250 words) with {config['display_name']}...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            summary_ids = current_model.generate(\n",
        "                inputs['input_ids'],\n",
        "                attention_mask=inputs['attention_mask'],\n",
        "                max_length=max_length,\n",
        "                min_length=min_length,\n",
        "                num_beams=config['num_beams'],\n",
        "                length_penalty=config['length_penalty'],\n",
        "                repetition_penalty=config['repetition_penalty'],  # HIGH - stops loops\n",
        "                no_repeat_ngram_size=config['no_repeat_ngram_size'],\n",
        "                early_stopping=True,\n",
        "                do_sample=False,\n",
        "            )\n",
        "\n",
        "        # ========== STEP 6: DECODING ==========\n",
        "        summary = current_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        word_count_raw = len(summary.split())\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Raw summary: {word_count_raw} words\")\n",
        "\n",
        "        # ========== STEP 7: ANTI-GARBAGE POST-PROCESSING ==========\n",
        "        had_garbage = detect_repetitive_garbage(summary)\n",
        "\n",
        "        if had_garbage:\n",
        "            if verbose:\n",
        "                print(f\"   ‚ö†Ô∏è  Detected repetitive garbage, removing...\")\n",
        "\n",
        "        summary = clean_summary_postprocessing(summary)\n",
        "\n",
        "        final_word_count = len(summary.split())\n",
        "\n",
        "        if had_garbage:\n",
        "            print(f\"‚ö†Ô∏è  Detected and removed repetitive garbage! ({word_count_raw} ‚Üí {final_word_count} words)\")\n",
        "\n",
        "        print(f\"‚úÖ Clean summary generated: {final_word_count} words!\")\n",
        "\n",
        "        return summary\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error generating summary: {str(e)}\"\n",
        "\n",
        "\n",
        "print(\"‚úÖ Summarization function ready!\")\n",
        "print(\"\\n‚öôÔ∏è FEATURES:\")\n",
        "print(\"   ‚Ä¢ Target: 150-250 words (concise & focused)\")\n",
        "print(\"   ‚Ä¢ High repetition penalty (2.5) - NO GARBAGE\")\n",
        "print(\"   ‚Ä¢ Automatic garbage detection & removal\")\n",
        "print(\"   ‚Ä¢ Complete sentences guaranteed\")\n",
        "print(\"   ‚Ä¢ Quality over quantity\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpncIbKupmEb",
        "outputId": "4efa0759-97a8-4daf-b529-9da4fa8a1b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Summarization function ready!\n",
            "\n",
            "‚öôÔ∏è FEATURES:\n",
            "   ‚Ä¢ Target: 150-250 words (concise & focused)\n",
            "   ‚Ä¢ High repetition penalty (2.5) - NO GARBAGE\n",
            "   ‚Ä¢ Automatic garbage detection & removal\n",
            "   ‚Ä¢ Complete sentences guaranteed\n",
            "   ‚Ä¢ Quality over quantity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 8: Test Summarization (150-250 WORDS, NO GARBAGE)\n",
        "# ============================================================\n",
        "\n",
        "import random\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üß™ TESTING CLEAN SUMMARY GENERATION (150-250 WORDS)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select random article\n",
        "random_idx = random.randint(0, len(dataset) - 1)\n",
        "test_article = dataset[random_idx]['article']\n",
        "test_reference = dataset[random_idx]['abstract']\n",
        "\n",
        "print(f\"\\nüìÑ Test Article (Index: {random_idx})\")\n",
        "print(f\"   Length: {len(test_article.split())} words\")\n",
        "print(f\"   Characters: {len(test_article)}\\n\")\n",
        "print(f\"   Preview: {test_article[:400]}...\\n\")\n",
        "\n",
        "print(f\"üìù Reference Summary ({len(test_reference.split())} words):\")\n",
        "print_wrapped(test_reference)\n",
        "print()\n",
        "\n",
        "# Generate with first model\n",
        "test_model = list(MODEL_CONFIGS.keys())[0]\n",
        "print(f\"ü§ñ Testing with {test_model}...\")\n",
        "print(f\"   Target: 150-250 words, clean output\\n\")\n",
        "\n",
        "generated = generate_summary(test_article, test_model, verbose=False)\n",
        "\n",
        "# Display result\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"‚ú® GENERATED SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print_wrapped(generated)\n",
        "print()\n",
        "\n",
        "# Quality checks\n",
        "word_count = len(generated.split())\n",
        "has_garbage = detect_repetitive_garbage(generated)\n",
        "is_complete = generated and generated[-1] in '.!?'\n",
        "in_range = 150 <= word_count <= 300\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üìä QUALITY CHECKS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"   Word count: {word_count}\")\n",
        "print(f\"   Target range (150-250): {'‚úÖ Yes' if in_range else '‚ö†Ô∏è  Outside range'}\")\n",
        "print(f\"   Has garbage: {'‚ùå Yes (PROBLEM!)' if has_garbage else '‚úÖ No (Clean!)'}\")\n",
        "print(f\"   Complete sentence: {'‚úÖ Yes' if is_complete else '‚ùå No'}\")\n",
        "print(f\"   Overall quality: {'‚úÖ EXCELLENT' if not has_garbage and in_range and is_complete else '‚ö†Ô∏è  Needs review'}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n‚úÖ Test complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT9-KxK5pq3e",
        "outputId": "c5bf5fe9-c3a3-4476-d011-3f67a9142249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ TESTING CLEAN SUMMARY GENERATION (150-250 WORDS)\n",
            "================================================================================\n",
            "\n",
            "üìÑ Test Article (Index: 3814)\n",
            "   Length: 2671 words\n",
            "   Characters: 15767\n",
            "\n",
            "   Preview: the inevitable exposure of salivary glands to radiation occurs frequently during radiotherapy of the head and neck region , which results in decreased saliva secretion , called xerostomia , shortly after a few radiation fractions . this may persist for the rest of the patient 's life , contributing to oral infections , caries and reduction in taste , and has been shown to be very prejudicial to th...\n",
            "\n",
            "üìù Reference Summary (233 words):\n",
            "the aim of this study was to evaluate the radioprotector effect of sodium\n",
            "selenite on the ultrastructure of submandibular glands in rats .   fifty - seven\n",
            "male albino wistar rats were randomized to 4 groups : control , irradiated ,\n",
            "sodium selenite and irradiated / sodium selenite .   the animals in the sodium\n",
            "selenite and irradiated / sodium selenite groups received intraperitoneal\n",
            "injections of sodium selenite ( 0.5 mg / kg body weight ) 24 h before\n",
            "irradiation .   the animals belonging to the irradiated and irradiated / sodium\n",
            "selenite groups were submitted to 15 gy of gamma radiation in the head and neck\n",
            "region .   the submandibular glands were removed at 4 , 8 , 12 , 24 , 48 and 72\n",
            "h after irradiation . the ionizing radiation induced damage to the secretory\n",
            "cells , especially the serous cells , right from the first period .\n",
            "vacuolization , lysis of cytoplasmic inclusions and nuclear alterations occurred\n",
            ".   the sodium selenite group also presented cellular alterations in the study\n",
            "periods , but with less damage compared to that caused by radiation .   there\n",
            "was greater similarity between the irradiated / sodium selenite group and the\n",
            "control group than with the other groups treated in all study periods . despite\n",
            "the alterations observed in the sodium selenite group , sodium selenite\n",
            "presented a radioprotective action on the secretory cells of submandibular\n",
            "glands .\n",
            "\n",
            "ü§ñ Testing with BART-PubMed (Balanced)...\n",
            "   Target: 150-250 words, clean output\n",
            "\n",
            "\n",
            "üì• Loading BART-PubMed from Drive...\n",
            "   Path: /content/drive/MyDrive/fine_tuned_bart\n",
            "‚úÖ BART-PubMed loaded successfully!\n",
            "\n",
            "ü§ñ Generating clean summary (150-250 words) with BART-PubMed...\n",
            "‚úÖ Clean summary generated: 51 words!\n",
            "\n",
            "================================================================================\n",
            "‚ú® GENERATED SUMMARY\n",
            "================================================================================\n",
            "purposethe aim of this study was to perform an ultrastructural evaluation of the\n",
            "radioprotective effect of sodium selenite on the damage caused by gamma\n",
            "radiation on the submandibular gland secretory cells in rats.materials and\n",
            "methodsfifty - seven 3-month - old male albino wistar rats, weighing 250 - 300 g\n",
            "were used.\n",
            "\n",
            "================================================================================\n",
            "üìä QUALITY CHECKS\n",
            "================================================================================\n",
            "   Word count: 51\n",
            "   Target range (150-250): ‚ö†Ô∏è  Outside range\n",
            "   Has garbage: ‚úÖ No (Clean!)\n",
            "   Complete sentence: ‚úÖ Yes\n",
            "   Overall quality: ‚ö†Ô∏è  Needs review\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Test complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cell 9: Gradio Interface (150-250 WORDS, NO GARBAGE)\n",
        "# ============================================================\n",
        "import gradio as gr\n",
        "\n",
        "# Prepare example articles\n",
        "example_articles = [\n",
        "    [dataset[11]['article'], \"PEGASUS-PubMed (Best Quality)\"],\n",
        "    [dataset[21]['article'], \"BART-PubMed (Balanced)\"],\n",
        "    [dataset[24]['article'], \"T5-PubMed (Fast)\"],\n",
        "    [dataset[37]['article'], \"PEGASUS-PubMed (Best Quality)\"],\n",
        "    [dataset[52]['article'], \"BART-PubMed (Balanced)\"],\n",
        "    [dataset[69]['article'], \"BART-PubMed (Balanced)\"],\n",
        "]\n",
        "\n",
        "\n",
        "def generate_summary_ui(article_text, model_name):\n",
        "    \"\"\"Gradio wrapper with quality verification.\"\"\"\n",
        "\n",
        "    # Generate summary\n",
        "    summary = generate_summary(article_text, model_name, verbose=False)\n",
        "\n",
        "    # If error occurred, return as-is\n",
        "    if summary.startswith(\"‚ö†Ô∏è\") or summary.startswith(\"‚ùå\"):\n",
        "        return summary\n",
        "\n",
        "    # Calculate statistics\n",
        "    word_count = len(summary.split())\n",
        "    char_count = len(summary)\n",
        "    input_words = len(article_text.split())\n",
        "    compression = input_words / word_count if word_count > 0 else 0\n",
        "    is_complete = summary and summary[-1] in '.!?'\n",
        "    has_garbage = detect_repetitive_garbage(summary)\n",
        "    in_range = 150 <= word_count <= 300\n",
        "\n",
        "    # Format output\n",
        "    result = f\"{summary}\\n\\n\"\n",
        "    result += f\"{'‚îÅ'*80}\\n\"\n",
        "    result += f\"üìä **SUMMARY STATISTICS**\\n\"\n",
        "    result += f\"{'‚îÅ'*80}\\n\"\n",
        "    result += f\"‚Ä¢ **Model**: {MODEL_CONFIGS[model_name]['display_name']}\\n\"\n",
        "    result += f\"‚Ä¢ **Summary length**: {word_count} words ({char_count} characters)\\n\"\n",
        "    result += f\"‚Ä¢ **Original length**: {input_words} words\\n\"\n",
        "    result += f\"‚Ä¢ **Compression ratio**: {compression:.1f}:1\\n\"\n",
        "    result += f\"‚Ä¢ **Target range (150-250)**: {'‚úì Yes' if in_range else '‚ö† Outside range'}\\n\"\n",
        "    result += f\"‚Ä¢ **Complete sentence**: {'‚úì Yes' if is_complete else '‚úó No'}\\n\"\n",
        "    result += f\"‚Ä¢ **Quality check**: {'‚úì Clean (no garbage)' if not has_garbage else '‚ö†Ô∏è Contains repetition'}\\n\"\n",
        "    result += f\"‚Ä¢ **Overall**: {'‚úÖ EXCELLENT QUALITY' if not has_garbage and is_complete else '‚ö†Ô∏è Needs review'}\"\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ LAUNCHING GRADIO WEB INTERFACE\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚è≥ Building interface...\\n\")\n",
        "\n",
        "# Create Gradio Interface\n",
        "with gr.Blocks(\n",
        "    title=\"Clean Biomedical Summarizer (150-250 Words)\",\n",
        "    theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"cyan\"),\n",
        ") as demo:\n",
        "\n",
        "    # Header\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üß¨ Clean Biomedical Article Summarizer\n",
        "\n",
        "    **Professional fine-tuned models** for high-quality biomedical literature summarization.\n",
        "\n",
        "    üéØ **3 optimized models** - PEGASUS, BART, T5\n",
        "    üìè **Concise 150-250 word summaries** - Focused, clean, no garbage\n",
        "    üõ°Ô∏è **Anti-garbage protection** - Guaranteed no repetitive loops\n",
        "    üìö **Trained on PubMed** - Medical domain expertise\n",
        "    ‚úÖ **Professional quality** - Similar to journal abstracts\n",
        "\n",
        "    ---\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # LEFT COLUMN\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### üìÑ Input Article\")\n",
        "\n",
        "            article_input = gr.Textbox(\n",
        "                lines=18,\n",
        "                placeholder=\"Paste your biomedical article here...\\n\\nMinimum 200 words recommended.\",\n",
        "                label=\"Biomedical Article Text\",\n",
        "                info=\"Enter or paste full biomedical article text\"\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### ‚öôÔ∏è Model Selection\")\n",
        "\n",
        "            model_dropdown = gr.Dropdown(\n",
        "                choices=list(MODEL_CONFIGS.keys()),\n",
        "                value=list(MODEL_CONFIGS.keys())[0],\n",
        "                label=\"Select Fine-Tuned Model\",\n",
        "                info=\"All models generate clean 150-250 word summaries\"\n",
        "            )\n",
        "\n",
        "            # Model info\n",
        "            first_model = list(MODEL_CONFIGS.keys())[0]\n",
        "            model_info = gr.Markdown(\n",
        "                f\"**{MODEL_CONFIGS[first_model]['display_name']}**\\n\\n\"\n",
        "                f\"{MODEL_CONFIGS[first_model]['description']}\\n\\n\"\n",
        "                f\"*Optimized parameters:*\\n\"\n",
        "                f\"- Target: 150-250 words\\n\"\n",
        "                f\"- Repetition penalty: {MODEL_CONFIGS[first_model]['repetition_penalty']} (HIGH)\\n\"\n",
        "                f\"- No garbage guarantee: ‚úÖ\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                generate_btn = gr.Button(\n",
        "                    \"üöÄ Generate Clean Summary\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\",\n",
        "                    scale=2\n",
        "                )\n",
        "                clear_btn = gr.Button(\"üóëÔ∏è Clear\", size=\"lg\", scale=1)\n",
        "\n",
        "        # RIGHT COLUMN\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### üìù Generated Summary\")\n",
        "\n",
        "            summary_output = gr.Textbox(\n",
        "                lines=22,\n",
        "                label=\"Clean Summary (150-250 words)\",\n",
        "                show_copy_button=True,\n",
        "                info=\"Concise, high-quality summary with no garbage\"\n",
        "            )\n",
        "\n",
        "    # Update model info when dropdown changes\n",
        "    def update_model_info(model_name):\n",
        "        config = MODEL_CONFIGS[model_name]\n",
        "        return (\n",
        "            f\"**{config['display_name']}**\\n\\n\"\n",
        "            f\"{config['description']}\\n\\n\"\n",
        "            f\"*Optimized parameters:*\\n\"\n",
        "            f\"- Target: 150-250 words\\n\"\n",
        "            f\"- Repetition penalty: {config['repetition_penalty']} (HIGH)\\n\"\n",
        "            f\"- No garbage guarantee: ‚úÖ\"\n",
        "        )\n",
        "\n",
        "    model_dropdown.change(\n",
        "        fn=update_model_info,\n",
        "        inputs=[model_dropdown],\n",
        "        outputs=[model_info]\n",
        "    )\n",
        "\n",
        "    # Examples\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"### üí° Example Articles from PubMed\")\n",
        "    gr.Markdown(\"*Click any example to generate a clean 150-250 word summary*\")\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=example_articles,\n",
        "        inputs=[article_input, model_dropdown],\n",
        "        outputs=summary_output,\n",
        "        fn=generate_summary_ui,\n",
        "        cache_examples=False,\n",
        "    )\n",
        "\n",
        "    # Instructions\n",
        "    with gr.Accordion(\"üìñ How to Use & Features\", open=False):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## How to Use\n",
        "\n",
        "        1. **Paste Article**: Copy your biomedical article (minimum 200 words)\n",
        "        2. **Select Model**: Choose based on your preference\n",
        "        3. **Generate**: Click button to get clean 150-250 word summary\n",
        "\n",
        "        ## Why 150-250 Words?\n",
        "\n",
        "        ‚úÖ **Standard abstract length** - Most PubMed abstracts are 200-300 words\n",
        "        ‚úÖ **Concise & focused** - Captures key information efficiently\n",
        "        ‚úÖ **Easy to read** - Quick understanding of the study\n",
        "        ‚úÖ **Professional** - Journal-quality output\n",
        "        ‚úÖ **No garbage** - Clean, coherent, complete sentences\n",
        "\n",
        "        ## Anti-Garbage Protection\n",
        "\n",
        "        All summaries are automatically protected:\n",
        "\n",
        "        ‚úÖ **High repetition penalties** (2.5) prevent loops\n",
        "        ‚úÖ **N-gram blocking** stops phrase repetition\n",
        "        ‚úÖ **Automatic detection** finds repetitive patterns\n",
        "        ‚úÖ **Smart removal** cuts before garbage starts\n",
        "        ‚úÖ **Quality verification** on every summary\n",
        "\n",
        "        ## Model Comparison\n",
        "\n",
        "        | Model | Quality | Speed | Output |\n",
        "        |-------|---------|-------|--------|\n",
        "        | **PEGASUS** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | 150-250 words |\n",
        "        | **BART** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | 150-250 words |\n",
        "        | **T5** | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | 150-250 words |\n",
        "\n",
        "        All models produce high-quality, clean summaries without garbage!\n",
        "        \"\"\")\n",
        "\n",
        "    # About\n",
        "    with gr.Accordion(\"‚ÑπÔ∏è About This System\", open=False):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### Clean Biomedical Summarization System\n",
        "\n",
        "        **Features:**\n",
        "        - üéØ 3 fine-tuned models (PEGASUS, BART, T5)\n",
        "        - üõ°Ô∏è Guaranteed no garbage output\n",
        "        - üìè Concise 150-250 word summaries\n",
        "        - ‚öôÔ∏è High repetition penalties (2.5)\n",
        "        - üìä Real-time quality verification\n",
        "        - üíæ Fast loading from Google Drive\n",
        "\n",
        "        **Focus:** Clean, professional, journal-quality summaries\n",
        "\n",
        "        **Technology:** Hugging Face Transformers, PyTorch, Gradio\n",
        "\n",
        "        ---\n",
        "\n",
        "        *Optimized for concise, high-quality biomedical summaries without repetitive garbage.*\n",
        "        \"\"\")\n",
        "\n",
        "    # Connect buttons\n",
        "    generate_btn.click(\n",
        "        fn=generate_summary_ui,\n",
        "        inputs=[article_input, model_dropdown],\n",
        "        outputs=summary_output,\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=lambda: (\"\", \"\"),\n",
        "        inputs=None,\n",
        "        outputs=[article_input, summary_output],\n",
        "    )\n",
        "\n",
        "# Launch\n",
        "print(\"üåê Starting Gradio server...\")\n",
        "demo.launch(share=True, debug=True, show_error=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ CLEAN SUMMARIZATION INTERFACE LAUNCHED!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚öôÔ∏è Configuration:\")\n",
        "print(\"   ‚Ä¢ Target: 150-250 words\")\n",
        "print(\"   ‚Ä¢ Repetition penalty: 2.5 (HIGH)\")\n",
        "print(\"   ‚Ä¢ No garbage guarantee: ‚úÖ\")\n",
        "print(\"   ‚Ä¢ Quality focus: Professional abstracts\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "P1hV2Rlipvok",
        "outputId": "5940c4d1-873c-405c-c5fa-16f8e111c20c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üöÄ LAUNCHING GRADIO WEB INTERFACE\n",
            "================================================================================\n",
            "‚è≥ Building interface...\n",
            "\n",
            "üåê Starting Gradio server...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3adb839f13e0578f3f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3adb839f13e0578f3f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ BART-PubMed (Balanced) already loaded\n",
            "\n",
            "ü§ñ Generating clean summary (150-250 words) with BART-PubMed...\n",
            "‚úÖ Clean summary generated: 111 words!\n",
            "\n",
            "üì• Loading PEGASUS-PubMed from Drive...\n",
            "   Path: /content/drive/MyDrive/fine_tuned_pegasus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjyO6TKnVQvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}